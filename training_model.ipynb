{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0331b9a0-2c40-4389-a570-5f9169a07393",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-21 16:29:38.786392: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-21 16:29:38.786421: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96090def-e16f-4287-b4f3-3162910b6c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "img_height = 250\n",
    "img_width = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61fdf01d-7902-441e-a5a3-6152105e8da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 75000 files belonging to 5 classes.\n",
      "Using 60000 files for training.\n",
      "Found 75000 files belonging to 5 classes.\n",
      "Using 15000 files for validation.\n"
     ]
    }
   ],
   "source": [
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  'rice_images/',\n",
    "  validation_split=0.2,\n",
    "  subset=\"training\",\n",
    "  seed=42,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  'rice_images/',\n",
    "  validation_split=0.2,\n",
    "  subset=\"validation\",\n",
    "  seed=42,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47051cbb-80fd-4030-aa50-a8da2751b8ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Arborio', 'Basmati', 'Ipsala', 'Jasmine', 'Karacadag']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names = train_ds.class_names\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33283402-729b-4822-9008-fb1be99a72bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54b1c4fe-e206-4e2f-a16a-9816b7a7d64e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-21 16:30:55.926175: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:390] Filling up shuffle buffer (this may take a while): 885 of 1000\n",
      "2022-05-21 16:30:57.231554: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:415] Shuffle buffer filled.\n",
      "2022-05-21 16:30:57.261571: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 1.0\n"
     ]
    }
   ],
   "source": [
    "normalization_layer = layers.Rescaling(1./255)\n",
    "normalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "image_batch, labels_batch = next(iter(normalized_ds))\n",
    "first_image = image_batch[0]\n",
    "# Notice the pixel values are now in `[0,1]`.\n",
    "print(np.min(first_image), np.max(first_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b104848-795f-4999-a172-b1eab707299c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(class_names)\n",
    "\n",
    "model = Sequential([\n",
    "  layers.Rescaling(1./255, input_shape=(img_height, img_width, 3)),\n",
    "  layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Flatten(),\n",
    "  layers.Dense(128, activation='relu'),\n",
    "  layers.Dense(num_classes)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5c69dac-651f-40ad-9eea-eb59ea05c313",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8fae5a9-2101-40ed-80e8-ed59b8c6437f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " rescaling_1 (Rescaling)     (None, 250, 250, 3)       0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 250, 250, 16)      448       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 125, 125, 16)     0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 125, 125, 32)      4640      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 62, 62, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 62, 62, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 31, 31, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 61504)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               7872640   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,896,869\n",
      "Trainable params: 7,896,869\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ecf75c-0473-41e5-8193-98728fe7226d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 184/7500 [..............................] - ETA: 58:06 - loss: 0.4438 - accuracy: 0.8390"
     ]
    }
   ],
   "source": [
    "epochs=10\n",
    "\n",
    "history = model.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=epochs\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
